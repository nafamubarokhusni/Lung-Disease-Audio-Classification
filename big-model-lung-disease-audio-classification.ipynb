{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":87331,"databundleVersionId":10191418,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lung Disease Classification menggunakan Video Vision Transformer\n<br>Nama : Husni Na'fa Mubarok\n<br>NIM : 121450078","metadata":{}},{"cell_type":"markdown","source":"## Import Package","metadata":{}},{"cell_type":"code","source":"import librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nimport librosa\nimport numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score\nfrom itertools import combinations\nfrom sklearn.preprocessing import LabelBinarizer\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nseed = 42\n\n# Set seed for TensorFlow\ntf.random.set_seed(seed)\n\n# Set seed for NumPy\nnp.random.seed(seed)\n\n# Set seed for Python's random module\nrandom.seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:01.367260Z","iopub.execute_input":"2025-04-21T13:43:01.368376Z","iopub.status.idle":"2025-04-21T13:43:01.375163Z","shell.execute_reply.started":"2025-04-21T13:43:01.368330Z","shell.execute_reply":"2025-04-21T13:43:01.374254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA and Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"### Import Dataframe & Processing","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/airs-ai-in-respiratory-sounds/train.csv\")\n\n# Candidate IDs to remove\nremove_ids = [\n    '5ee582f2832c2',]\n#     'd3ef8479c4212',\n#     '912eb10321597',\n#     'f0d3c857bec13',\n#     '20d7c44ffabbb',\n#     '2d1645a4f4db8'\n# ]\n\n# Filter out the unwanted candidateIDs\ntrain = train[~train['candidateID'].isin(remove_ids)]\n\n# Print info and null value summary\nprint(train.info())\nprint(train.isnull().sum())\n\n# Display the updated dataframe\ntrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:01.376634Z","iopub.execute_input":"2025-04-21T13:43:01.376979Z","iopub.status.idle":"2025-04-21T13:43:01.419171Z","shell.execute_reply.started":"2025-04-21T13:43:01.376949Z","shell.execute_reply":"2025-04-21T13:43:01.418402Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Drop Null Column","metadata":{}},{"cell_type":"code","source":"train = train.drop(columns=[\"coldPresent\"])\ntrain.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:01.420460Z","iopub.execute_input":"2025-04-21T13:43:01.420667Z","iopub.status.idle":"2025-04-21T13:43:01.427422Z","shell.execute_reply.started":"2025-04-21T13:43:01.420651Z","shell.execute_reply":"2025-04-21T13:43:01.426628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Sound Visualization & Processing","metadata":{}},{"cell_type":"code","source":"# Define sound folder and candidate IDs\nSOUND_FOLDER = \"/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds\"\nOUTPUT_FOLDER = \"/kaggle/working/waveform_images\"  # Folder to save images\nos.makedirs(OUTPUT_FOLDER, exist_ok=True)  # Create the folder if it doesn't exist\n\ncandidate_ids = [\"b87ea0dd760fa\", \"e000a41725f53\", \"253ba780a0398\"]  # List of candidate IDs\n\n# Generate the list of audio file paths\naudio_paths = [os.path.join(SOUND_FOLDER, candidate_id, \"cough.wav\") for candidate_id in candidate_ids]\n\n# Create subplots for multiple waveforms\nfig, axes = plt.subplots(len(audio_paths), 1, figsize=(10, 4 * len(audio_paths)))\n\n# Ensure axes is iterable for a single file\nif len(audio_paths) == 1:\n    axes = [axes]  \n\nfor i, file in enumerate(audio_paths):\n    try:\n        y, sr = librosa.load(file, sr=None)  # Load audio file\n        librosa.display.waveshow(y, sr=sr, ax=axes[i])  # Plot waveform\n        axes[i].set_title(f\"Waveform dari {candidate_ids[i]}\")  # Use candidate ID in title\n        axes[i].set_xlabel(\"Waktu (s)\")\n        axes[i].set_ylabel(\"Amplitudo\")\n    except Exception as e:\n        print(f\"Error loading {file}: {e}\")\n\n# Save the figure\nsave_path = os.path.join(OUTPUT_FOLDER, \"waveforms.png\")\nplt.savefig(save_path, dpi=300, bbox_inches='tight')\nprint(f\"Waveform image saved to: {save_path}\")\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:01.428392Z","iopub.execute_input":"2025-04-21T13:43:01.428584Z","iopub.status.idle":"2025-04-21T13:43:16.096987Z","shell.execute_reply.started":"2025-04-21T13:43:01.428560Z","shell.execute_reply":"2025-04-21T13:43:16.096095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Split & Pipeline","metadata":{}},{"cell_type":"code","source":"SOUND_FOLDER = \"/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds\"\n\ndata = train\nN_MFCC = 20\nBATCH_SIZE = 32\nSEQ_LENGTH = 512\nEMBED_DIM = 512\nNUM_HEADS = 8\nNUM_LAYERS = 6\nNUM_FEATURE = len(data.columns) - 2\nD_MODELS = EMBED_DIM\nEPOCHS = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:16.098302Z","iopub.execute_input":"2025-04-21T13:43:16.098829Z","iopub.status.idle":"2025-04-21T13:43:16.103272Z","shell.execute_reply.started":"2025-04-21T13:43:16.098808Z","shell.execute_reply":"2025-04-21T13:43:16.102518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def segment_cough_sound(signal, sr, cough_threshold=0.05, min_cough_duration=0.1, padding=0.05, visualize=True):\n    hop_length = int(min_cough_duration * sr)\n    if len(signal.shape) > 1:\n        signal = np.mean(signal, axis=1)\n\n    energy = librosa.feature.rms(y=signal, hop_length=hop_length)[0]\n\n    # Normalize the energy values\n    normalized_energy = (energy - np.min(energy)) / (np.max(energy) - np.min(energy))\n\n    # Set the energy threshold for event detection\n    energy_threshold = np.max(normalized_energy) * cough_threshold\n    min_cough_samples = round(sr * min_cough_duration)\n\n    # Find the cough segments\n    cough_segments = []\n    segments_time = []\n    event_start = None\n\n    for i, value in enumerate(normalized_energy):\n        if value >= energy_threshold:\n            if event_start is None:\n                event_start = i * hop_length\n        else:\n            if event_start is not None:\n                cough_duration = i * hop_length - event_start\n                if cough_duration >= min_cough_samples:\n                    event_end = i * hop_length + int(padding * sr)\n                    event_start_pad = max(event_start - int(padding * sr), 0)\n                    event_end = min(event_end, len(signal) - 1)\n                    cough_segments.append(signal[event_start_pad: event_end + 1])\n                    segments_time.append((event_start_pad / sr, event_end / sr))\n                event_start = None\n\n    if visualize:\n        times = np.linspace(0, len(signal) / sr, num=len(signal))\n\n        plt.figure(figsize=(14, 5))\n        plt.plot(times, signal, label=\"Audio Signal\")\n        for start, end in segments_time:\n            plt.axvspan(start, end, color='red', alpha=0.3, label=\"Cough Segment\" if 'Cough Segment' not in plt.gca().get_legend_handles_labels()[1] else None)\n\n        plt.title(\"Cough Sound Segmentation\")\n        plt.xlabel(\"Time (s)\")\n        plt.ylabel(\"Amplitude\")\n        plt.legend(loc='upper right')\n        plt.tight_layout()\n        plt.show()\n\n    return cough_segments\n\ny, sr = librosa.load(\"/kaggle/input/airs-ai-in-respiratory-sounds/sounds/sounds/e000a41725f53/cough.wav\")\nvisual_audio = segment_cough_sound(y, sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:16.104159Z","iopub.execute_input":"2025-04-21T13:43:16.104910Z","iopub.status.idle":"2025-04-21T13:43:17.635835Z","shell.execute_reply.started":"2025-04-21T13:43:16.104891Z","shell.execute_reply":"2025-04-21T13:43:17.635046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def segment_cough_sound(signal, sr, cough_threshold=0.05, min_cough_duration=0.1, padding=0.05):\n\n    hop_length = int(min_cough_duration*sr)\n    if len(signal.shape) > 1:\n        signal = np.mean(signal, axis=1)\n\n    energy = librosa.feature.rms(y=signal, hop_length=hop_length)[0]\n\n    # Normalize the energy values\n    normalized_energy = (energy - np.min(energy)) / (np.max(energy) - np.min(energy))\n\n    # Set the energy threshold for event detection\n    cough_threshold = np.max(normalized_energy) * cough_threshold\n    min_cough_samples = round(sr * min_cough_duration)\n\n\n    # Find the cough segments\n    cough_segments = []\n    event_start = None\n\n    for i, value in enumerate(normalized_energy):\n        if value >= cough_threshold:\n            if event_start is None:\n                event_start = i*hop_length\n        else:\n            if event_start is not None:\n                cough_duration = i*hop_length - event_start\n                if cough_duration >= min_cough_samples:\n                    event_end = i*hop_length + int(padding * sr)\n                    event_start -= int(padding * sr)\n                    event_start = max(event_start, 0)\n                    cough_segments.append(signal[event_start: event_end+1])\n                event_start = None\n\n    # Convert cough segments to time in seconds\n    # cough_segments = [(start / sr, end / sr) for start, end in cough_segments]\n\n    return cough_segments\n\ndef extract_mfcc(file_path, n_mfcc=N_MFCC, target_length=SEQ_LENGTH):\n    try:\n        audio, sr = librosa.load(file_path)\n        cough_segments = segment_cough_sound(audio, sr)\n\n        # If no cough segments found, use full audio\n        if not cough_segments:\n            print(f\"No cough segments detected for {file_path}, using full signal.\")\n            segmented_audio = audio\n        else:\n            segmented_audio = np.concatenate(cough_segments)\n\n        # Extract MFCC from the segmented audio\n        mfcc = librosa.feature.mfcc(y=segmented_audio, sr=sr, n_mfcc=n_mfcc)\n        mfcc = mfcc.T  # shape: (time_steps, n_mfcc)\n\n        # Pad or trim to target length\n        if len(mfcc) > target_length:\n            mfcc = mfcc[:target_length]\n        elif len(mfcc) < target_length:\n            pad_width = target_length - len(mfcc)\n            mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n\n        return tf.convert_to_tensor(mfcc, dtype=tf.float32)\n\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n        return tf.zeros((target_length, n_mfcc), dtype=tf.float32)\n\n\ndef preprocess_features(row):\n    age = row[\"age\"] / 100.0\n    pack_years = row[\"packYears\"] / 100.0\n    gender = row[\"gender\"]\n    tb_contact_history = row[\"tbContactHistory\"]\n    wheezing_history = row[\"wheezingHistory\"]\n    phlegm_cough = row[\"phlegmCough\"]\n    family_asthma_history = row[\"familyAsthmaHistory\"]\n\n    fever_history = row[\"feverHistory\"]\n\n    features = [\n        age, gender, pack_years, tb_contact_history, wheezing_history,\n        phlegm_cough, family_asthma_history, fever_history\n    ]\n    features = np.array(features).reshape(-1, 1)\n    return features\n\ndef process_row(row):\n    candidate_id = row[\"candidateID\"]\n    audio_path = os.path.join(SOUND_FOLDER, str(candidate_id), \"cough.wav\")\n    mfcc = extract_mfcc(audio_path)\n    features = preprocess_features(row)\n    label = row[\"disease\"]\n    label = tf.one_hot(label, depth=3)\n    return mfcc, features, label\n\ndef data_generator(data):\n    for _, row in data.iterrows():\n        mfcc, features, label = process_row(row)\n        yield mfcc, features, label\n\ndef create_dataset(data, batch_size=BATCH_SIZE):\n    output_signature = (\n        tf.TensorSpec(shape=(SEQ_LENGTH, N_MFCC), dtype=tf.float32),\n        tf.TensorSpec(shape=(NUM_FEATURE, 1), dtype=tf.float32),\n        tf.TensorSpec(shape=(3,), dtype=tf.int32),\n    )\n    dataset = tf.data.Dataset.from_generator(\n        lambda: data_generator(data),\n        output_signature=output_signature\n    )\n    return dataset.batch(batch_size).shuffle(256).prefetch(tf.data.AUTOTUNE)\n\ndef split_data(data, test_size=0.2, random_state=42):\n    train_data, valid_data = train_test_split(\n        data, test_size=test_size, random_state=random_state, stratify=data['disease']\n    )\n    return train_data, valid_data\n\ntrain_data_df, valid_data_df = split_data(data)\n\ntrain_dataset = create_dataset(train_data_df, batch_size=BATCH_SIZE)\nvalid_dataset = create_dataset(valid_data_df, batch_size=BATCH_SIZE)\ntrain_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:17.636579Z","iopub.execute_input":"2025-04-21T13:43:17.637104Z","iopub.status.idle":"2025-04-21T13:43:19.258769Z","shell.execute_reply.started":"2025-04-21T13:43:17.637084Z","shell.execute_reply":"2025-04-21T13:43:19.258181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # List of candidateIDs to check\n# check_ids = [\n#     '5ee582f2832c2',\n#     'd3ef8479c4212',\n#     '912eb10321597',\n#     'f0d3c857bec13',\n#     '20d7c44ffabbb',\n#     '2d1645a4f4db8'\n# ]\n\n# # Check if any of these IDs are still in train_data_df\n# remaining_ids = train_data_df[train_data_df['candidateID'].isin(check_ids)]\n\n# # Display the result\n# print(f\"Remaining candidateIDs in train_data_df:\\n{remaining_ids}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:19.259400Z","iopub.execute_input":"2025-04-21T13:43:19.259580Z","iopub.status.idle":"2025-04-21T13:43:19.263356Z","shell.execute_reply.started":"2025-04-21T13:43:19.259565Z","shell.execute_reply":"2025-04-21T13:43:19.262613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transformer Component","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self, sequence_length, embed_dim, trainable_embed=False, **kwargs):\n        super().__init__(**kwargs)\n        \n        # Generate positional encoding matrix\n        position_embedding_matrix = self.get_position_encoding(sequence_length, embed_dim)\n        \n        # Non-trainable Embedding layer to store positional encodings\n        if trainable_embed == False:\n            self.position_embedding_layer = layers.Embedding(\n                input_dim=sequence_length, output_dim=embed_dim,\n                weights=[position_embedding_matrix],\n                trainable=False\n            )\n        else:\n            self.position_embedding_layer = layers.Embedding(\n                input_dim=sequence_length, output_dim=embed_dim,\n                trainable=True\n            )\n             \n    def get_position_encoding(self, seq_len, d, n=10000):\n        P = np.zeros((seq_len, d))\n        for k in range(seq_len):\n            for i in np.arange(int(d/2)):\n                denominator = np.power(n, 2*i/d)\n                P[k, 2*i] = np.sin(k/denominator)\n                P[k, 2*i+1] = np.cos(k/denominator)\n        return P\n \n    def call(self, inputs):        \n        # Get position indices (0, 1, ..., sequence_length-1)\n        sequence_length = tf.shape(inputs)[1]  # Ambil sequence_length dari inputs\n        position_indices = tf.range(sequence_length)  # Shape: (sequence_length,)\n        \n        # Get positional encoding\n        positional_encoding = self.position_embedding_layer(position_indices)  # Shape: (sequence_length, embed_dim)\n        \n        # Expand dimensions for broadcasting: (1, sequence_length, embed_dim)\n        return positional_encoding[tf.newaxis, :, :]\n\n\nclass TransformerEncoderBlock(tf.keras.Model):\n    def __init__(self, d_models, num_heads, num_layers=6, **kwargs):\n        super().__init__(**kwargs)\n\n        self.dense256 = layers.Dense(256, activation=\"relu\")\n        self.dense_transform = layers.Dense(d_models, activation=\"relu\")\n        self.num_layers = num_layers\n        self.linear = layers.Dense(d_models, activation=\"relu\")\n\n        self.frame_positional_encoding = PositionalEncoding(\n            sequence_length=NUM_FEATURE + 1, embed_dim=d_models, trainable_embed=True\n        )\n        self.patch_positional_encoding = PositionalEncoding(\n            sequence_length=SEQ_LENGTH + 1, embed_dim=d_models, trainable_embed=True\n        )\n\n        # Define layers as lists\n        self.attention_frame = [layers.MultiHeadAttention(num_heads=num_heads, key_dim=int(d_models/num_heads), dropout=0.2) \n                                for _ in range(num_layers)]\n        self.attention_cross = [layers.MultiHeadAttention(num_heads=num_heads, key_dim=int(d_models/num_heads), dropout=0.2) \n                                for _ in range(num_layers)]\n        self.layernorm_frame = [layers.LayerNormalization() for _ in range(num_layers)]\n        self.layernorm_cross = [layers.LayerNormalization() for _ in range(num_layers)]\n        self.dense_1 = [layers.Dense(d_models*2, activation=\"gelu\") for _ in range(num_layers)]\n        self.dense_2 = [layers.Dense(d_models) for _ in range(num_layers)]\n        self.dropout = [layers.Dropout(0.2) for _ in range(num_layers)]\n\n        # Learnable class tokens\n        self.cls_token = self.add_weight(\n            shape=(1, 1, d_models),\n            initializer=\"random_normal\",\n            trainable=True,\n            name=\"cls_token\"\n        )\n\n        self.cls_token2 = self.add_weight(\n            shape=(1, 1, d_models),\n            initializer=\"random_normal\",\n            trainable=True,\n            name=\"cls_token2\"\n        )\n\n        # Final output layers\n        self.out = layers.Dense(d_models, activation=\"gelu\")\n        self.out_drop = layers.Dropout(0.2)\n\n    def call(self, inputs, training=False):\n        \"\"\"Transformer Encoder Forward Pass\"\"\"\n        inputs, frame = inputs\n\n        # Add class token\n        batch_size = tf.shape(inputs)[0]\n        inputs = self.dense_transform(self.dense256(inputs))\n        cls_tokens = tf.broadcast_to(self.cls_token, [batch_size, 1, tf.shape(self.cls_token)[-1]])\n        cls_tokens2 = tf.broadcast_to(self.cls_token2, [batch_size, 1, tf.shape(self.cls_token2)[-1]])\n\n        # Process frame\n        frame = self.linear(frame)\n        frame = tf.concat([cls_tokens2, frame], axis=1)\n        Zt = layers.Add()([frame, self.frame_positional_encoding(frame)])\n\n        # Process inputs\n        inputs = tf.concat([cls_tokens, inputs], axis=1)\n        Zs = layers.Add()([inputs, self.patch_positional_encoding(inputs)])\n\n        # Iterate over pre-defined layers (avoid Graph Mode error)\n        for i in range(self.num_layers):\n            Zt = self.layernorm_frame[i](Zt + self.attention_frame[i](query=Zt, value=Zt, key=Zt, training=training))\n            Zts = tf.concat([Zs, Zt], axis=1)\n            Zs = self.layernorm_cross[i](Zs + self.attention_cross[i](query=Zs, value=Zts, key=Zts, training=training))\n\n            inputs = self.dense_1[i](Zs)\n            inputs = self.dropout[i](inputs, training=training)\n            inputs = self.dense_2[i](inputs)\n            Zs += inputs\n\n            inputsf = self.dense_1[i](Zt)\n            inputsf = self.dropout[i](inputsf, training=training)\n            inputsf = self.dense_2[i](inputsf)\n            Zt += inputsf\n\n        # Output\n        Zt = Zt[:, 0, :]\n        Zs = Zs[:, 0, :]\n        out = tf.concat([Zs, Zt], axis=-1)\n        out = self.out(out)\n        out = self.out_drop(out)\n        return out\n\n\n    def create_look_ahead_mask(self, size):\n        return tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n\n\nclass Classifier(tf.keras.Model):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.densel = layers.Dense(EMBED_DIM)\n        self.out = layers.Dense(3, activation=\"softmax\")\n\n    def call(self, inputs):\n        x = self.densel(inputs)\n        x = self.out(x)\n        return x\n\n\n\nclass MainModel(keras.Model):\n    def __init__(self, encoder, classifier, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.classifier = classifier\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n\n    def call(self, inputs, training=False):\n        \"\"\"Forward pass through encoder and classifier\"\"\"\n        encoder_out = self.encoder(inputs, training=training)\n        preds = self.classifier(encoder_out)\n        return preds  # Returns predicted probabilities\n\n    def calculate_loss(self, y_pred, y_true):\n        return self.loss(y_true, y_pred)\n\n    def calculate_acc(self, y_pred, y_true):\n        predicted_classes = tf.argmax(y_pred, axis=1)\n        y_true = tf.argmax(y_true, axis=1)\n        accuracy = tf.reduce_mean(tf.cast(tf.equal(y_true, predicted_classes), dtype=tf.float32))\n        return accuracy\n\n    def train_step(self, batch_data):\n        batch, batch2, label = batch_data\n\n        with tf.GradientTape() as tape:\n            preds = self(inputs=[batch, batch2], training=True)\n            batch_loss = self.calculate_loss(preds, label)\n            batch_acc = self.calculate_acc(preds, label)\n\n        train_vars = self.trainable_variables\n        grads = tape.gradient(batch_loss, train_vars)\n        self.optimizer.apply_gradients(zip(grads, train_vars))\n\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.acc_tracker.result()}\n\n    def test_step(self, batch_data):\n        batch, batch2, label = batch_data\n        preds = self(inputs=[batch, batch2], training=False)\n        batch_loss = self.calculate_loss(preds, label)\n        batch_acc = self.calculate_acc(preds, label)\n\n        self.loss_tracker.update_state(batch_loss)\n        self.acc_tracker.update_state(batch_acc)\n\n        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.acc_tracker.result()}\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker, self.acc_tracker]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:19.264063Z","iopub.execute_input":"2025-04-21T13:43:19.264257Z","iopub.status.idle":"2025-04-21T13:43:19.288352Z","shell.execute_reply.started":"2025-04-21T13:43:19.264242Z","shell.execute_reply":"2025-04-21T13:43:19.287759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"encoder = TransformerEncoderBlock(d_models=D_MODELS, num_heads=NUM_HEADS, num_layers=NUM_LAYERS)\nclassifier = Classifier()\nmodel = MainModel(encoder, classifier)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:19.289006Z","iopub.execute_input":"2025-04-21T13:43:19.289199Z","iopub.status.idle":"2025-04-21T13:43:20.614205Z","shell.execute_reply.started":"2025-04-21T13:43:19.289184Z","shell.execute_reply":"2025-04-21T13:43:20.613420Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super().__init__()\n\n        self.d_model = tf.cast(d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        step = tf.cast(step, dtype=tf.float32)\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        \n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\n    def get_config(self):\n        return {\n            \"d_model\": self.d_model.numpy(),  # Konversi ke tipe Python agar bisa diserialisasi\n            \"warmup_steps\": self.warmup_steps\n        }\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:20.616703Z","iopub.execute_input":"2025-04-21T13:43:20.616970Z","iopub.status.idle":"2025-04-21T13:43:20.622331Z","shell.execute_reply.started":"2025-04-21T13:43:20.616953Z","shell.execute_reply":"2025-04-21T13:43:20.621616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the loss function\ncross_entropy = keras.losses.CategoricalCrossentropy()\n\n# EarlyStopping criteria\nearly_stopping = keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode='min', patience=15, restore_best_weights=True)\n#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0000001)\n# Optimizer\nlr = CustomSchedule(D_MODELS)\n# optimizer = tf.keras.optimizers.AdamW(\n#     learning_rate=0.001,\n#     weight_decay=0.004,\n#     beta_1=0.9,\n#     beta_2=0.999,\n#     epsilon=1e-07,\n# )\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=lr,\n    beta_1=0.9,\n    beta_2=0.98,\n    weight_decay=1e-5,\n    epsilon=1e-9\n)\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss=cross_entropy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:50.068822Z","iopub.execute_input":"2025-04-21T13:43:50.069096Z","iopub.status.idle":"2025-04-21T13:43:50.078137Z","shell.execute_reply.started":"2025-04-21T13:43:50.069077Z","shell.execute_reply":"2025-04-21T13:43:50.077579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:50.090124Z","iopub.execute_input":"2025-04-21T13:43:50.090293Z","iopub.status.idle":"2025-04-21T13:43:50.103638Z","shell.execute_reply.started":"2025-04-21T13:43:50.090280Z","shell.execute_reply":"2025-04-21T13:43:50.102968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Fit the model\nhistory = model.fit(train_dataset, epochs=EPOCHS, validation_data=valid_dataset, callbacks=[early_stopping])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:43:50.105018Z","iopub.execute_input":"2025-04-21T13:43:50.105204Z","iopub.status.idle":"2025-04-21T14:09:08.101154Z","shell.execute_reply.started":"2025-04-21T13:43:50.105190Z","shell.execute_reply":"2025-04-21T14:09:08.100491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize Training Accuracy & Loss","metadata":{}},{"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.figure(figsize=(12, 8))\nplt.plot(history.history['accuracy'], marker='o', linestyle='-')\nplt.plot(history.history['val_accuracy'], marker='o', linestyle='--')\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.grid(True)\n\n# Save the accuracy plot\nplt.savefig(\"training_validation_accuracy.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n# Plot training & validation loss values\nplt.figure(figsize=(12, 8))\nplt.plot(history.history['loss'], marker='o', linestyle='-')\nplt.plot(history.history['val_loss'], marker='o', linestyle='--')\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.grid(True)\n\n# Save the loss plot\nplt.savefig(\"training_validation_loss.png\", dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:08.102051Z","iopub.execute_input":"2025-04-21T14:09:08.102262Z","iopub.status.idle":"2025-04-21T14:09:09.535091Z","shell.execute_reply.started":"2025-04-21T14:09:08.102244Z","shell.execute_reply":"2025-04-21T14:09:09.534310Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Save Model","metadata":{}},{"cell_type":"code","source":"# Save the entire model as a `.keras` zip archive.\nmodel.save('big_model_lung_disease_classification_transformer.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:09.535981Z","iopub.execute_input":"2025-04-21T14:09:09.536226Z","iopub.status.idle":"2025-04-21T14:09:10.645887Z","shell.execute_reply.started":"2025-04-21T14:09:09.536208Z","shell.execute_reply":"2025-04-21T14:09:10.645309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_weights(\"model_weights.weights.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:10.647392Z","iopub.execute_input":"2025-04-21T14:09:10.647608Z","iopub.status.idle":"2025-04-21T14:09:11.270491Z","shell.execute_reply.started":"2025-04-21T14:09:10.647591Z","shell.execute_reply":"2025-04-21T14:09:11.269940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# def extract_mfcc(file_path, n_mfcc=N_MFCC, target_length=SEQ_LENGTH):\n#     try:\n#         audio, sr = librosa.load(file_path)\n#         mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n#         mfcc = mfcc.T\n#         if len(mfcc) > target_length:\n#             mfcc = mfcc[:target_length]\n#         elif len(mfcc) < target_length:\n#             pad_width = target_length - len(mfcc)\n#             mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n#         return mfcc\n#     except Exception as e:\n#         print(f\"Error processing {file_path}: {e}\")\n\ndef extract_mfcc(file_path, n_mfcc=N_MFCC, target_length=SEQ_LENGTH):\n    try:\n        audio, sr = librosa.load(file_path)\n        cough_segments = segment_cough_sound(audio, sr)\n\n        # If no cough segments found, use full audio\n        if not cough_segments:\n            print(f\"No cough segments detected for {file_path}, using full signal.\")\n            segmented_audio = audio\n        else:\n            segmented_audio = np.concatenate(cough_segments)\n\n        # Extract MFCC from the segmented audio\n        mfcc = librosa.feature.mfcc(y=segmented_audio, sr=sr, n_mfcc=n_mfcc)\n        mfcc = mfcc.T  # shape: (time_steps, n_mfcc)\n\n        # Pad or trim to target length\n        if len(mfcc) > target_length:\n            mfcc = mfcc[:target_length]\n        elif len(mfcc) < target_length:\n            pad_width = target_length - len(mfcc)\n            mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n\n        return tf.convert_to_tensor(mfcc, dtype=tf.float32)\n\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n        return tf.zeros((target_length, n_mfcc), dtype=tf.float32)\n\ndef preprocess_features(row):\n    age = row[\"age\"] / 100.0\n    pack_years = row[\"packYears\"] / 100.0\n    gender = row[\"gender\"]\n    tb_contact_history = row[\"tbContactHistory\"]\n    wheezing_history = row[\"wheezingHistory\"]\n    phlegm_cough = row[\"phlegmCough\"]\n    family_asthma_history = row[\"familyAsthmaHistory\"]\n\n    fever_history = row[\"feverHistory\"]\n\n    features = [\n        age, gender, pack_years, tb_contact_history, wheezing_history,\n        phlegm_cough, family_asthma_history, fever_history\n    ]\n    features = np.array(features).reshape(-1, 1)\n    return features\n\ndef process_row(row):\n    candidate_id = row[\"candidateID\"]\n    audio_path = os.path.join(SOUND_FOLDER, str(candidate_id), \"cough.wav\")\n    mfcc = extract_mfcc(audio_path)\n    features = preprocess_features(row)\n    mfcc = tf.convert_to_tensor(mfcc, dtype=tf.float32)\n    features = tf.convert_to_tensor(features, dtype=tf.float32)\n    mfcc.set_shape((SEQ_LENGTH, N_MFCC))\n    features.set_shape((NUM_FEATURE, 1))\n    mfcc = tf.expand_dims(mfcc, axis=0)\n    features = tf.expand_dims(features, axis=0)\n    return mfcc, features\n\ndef predict(model, mfcc, features):\n    encoder_out = model.encoder([mfcc, features], training=False)\n    pred = model.classifier(encoder_out)\n    predicted_classes = tf.argmax(pred, axis=1)\n    return int(predicted_classes[0]), pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:11.271349Z","iopub.execute_input":"2025-04-21T14:09:11.271629Z","iopub.status.idle":"2025-04-21T14:09:11.281394Z","shell.execute_reply.started":"2025-04-21T14:09:11.271609Z","shell.execute_reply":"2025-04-21T14:09:11.280700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_dataset_auc = valid_data_df.copy()\nvalid_dataset_auc = valid_dataset_auc.drop(columns=[\"disease\"])\nvalid_dataset_dict = valid_dataset_auc.to_dict(orient = 'records')\n# List untuk menyimpan hasil prediksi\nresults = []\n\nfor row in valid_dataset_dict:\n    mfcc, features = process_row(row)  # Proses data\n    prediction = predict(model, mfcc, features)  # Prediksi\n\n    # Tambahkan hasil ke list\n    results.append({\n        \"candidateID\": row[\"candidateID\"],\n        \"y_score\": prediction[1].numpy().tolist(),\n        \"disease\": prediction[0]\n    })\n\n# Konversi hasil ke DataFrame\ny_pred_prob = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:11.282171Z","iopub.execute_input":"2025-04-21T14:09:11.282392Z","iopub.status.idle":"2025-04-21T14:09:35.967624Z","shell.execute_reply.started":"2025-04-21T14:09:11.282370Z","shell.execute_reply":"2025-04-21T14:09:35.967031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_test = valid_data_df['disease']\ny_score = y_pred_prob['y_score']\ny_score = np.asarray(y_score)\ny_score = np.array([item[0] for item in y_score])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:35.968344Z","iopub.execute_input":"2025-04-21T14:09:35.968536Z","iopub.status.idle":"2025-04-21T14:09:35.972883Z","shell.execute_reply.started":"2025-04-21T14:09:35.968521Z","shell.execute_reply":"2025-04-21T14:09:35.972266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Confusion matrix","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred_prob['disease'], labels=[0, 1, 2])\n\n# Display it\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix - 3 Class\")\nplt.show()\nplt.savefig('Confusion Matrix - 3 Class.png', dpi=300, bbox_inches='tight')  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:35.973547Z","iopub.execute_input":"2025-04-21T14:09:35.973835Z","iopub.status.idle":"2025-04-21T14:09:36.285289Z","shell.execute_reply.started":"2025-04-21T14:09:35.973811Z","shell.execute_reply":"2025-04-21T14:09:36.284656Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Accuracy, Precision, Recall, F1 Score","metadata":{}},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred_prob['disease'])\nprecision = precision_score(y_test, y_pred_prob['disease'], average='macro')\nrecall = recall_score(y_test, y_pred_prob['disease'], average='macro')\nf1 = f1_score(y_test, y_pred_prob['disease'], average='macro')\n\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:36.285999Z","iopub.execute_input":"2025-04-21T14:09:36.286212Z","iopub.status.idle":"2025-04-21T14:09:36.299105Z","shell.execute_reply.started":"2025-04-21T14:09:36.286187Z","shell.execute_reply":"2025-04-21T14:09:36.298494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ROC Curve","metadata":{}},{"cell_type":"code","source":"pair_list = list(combinations(np.unique(y_pred_prob['disease']), 2))\nprint(pair_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:36.299902Z","iopub.execute_input":"2025-04-21T14:09:36.300151Z","iopub.status.idle":"2025-04-21T14:09:36.306002Z","shell.execute_reply.started":"2025-04-21T14:09:36.300128Z","shell.execute_reply":"2025-04-21T14:09:36.305396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fpr_grid = np.linspace(0.0, 1.0, 1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:36.306653Z","iopub.execute_input":"2025-04-21T14:09:36.306903Z","iopub.status.idle":"2025-04-21T14:09:36.322426Z","shell.execute_reply.started":"2025-04-21T14:09:36.306886Z","shell.execute_reply":"2025-04-21T14:09:36.321924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_binarizer = LabelBinarizer().fit(train_data_df['disease'])\n\npair_scores = []\nmean_tpr = dict()\n\n# Make sure the output directory exists\nos.makedirs(\"roc_plots\", exist_ok=True)\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = np.logical_or(a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = np.flatnonzero(label_binarizer.classes_ == label_a)[0]\n    idx_b = np.flatnonzero(label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = roc_curve(a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = roc_curve(b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = np.zeros_like(fpr_grid)\n    mean_tpr[ix] += np.interp(fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += np.interp(fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = auc(fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    plt.plot(\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    RocCurveDisplay.from_predictions(\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    RocCurveDisplay.from_predictions(\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ax.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\",\n        title=f\"{label_a} vs {label_b} ROC curves\",\n    )\n    ax.legend(loc=\"lower right\")\n\n    # Save the figure\n    filename = f\"roc_plots/roc_{label_a}_vs_{label_b}.png\"\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close(fig)  # Close to free memory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:36.324408Z","iopub.execute_input":"2025-04-21T14:09:36.324603Z","iopub.status.idle":"2025-04-21T14:09:37.684283Z","shell.execute_reply.started":"2025-04-21T14:09:36.324588Z","shell.execute_reply":"2025-04-21T14:09:37.683409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predict New Data","metadata":{}},{"cell_type":"markdown","source":"### Data Processing","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/airs-ai-in-respiratory-sounds/test.csv\")\nprint(test.info())\nprint(test.isnull().sum())\ntest","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:37.685089Z","iopub.execute_input":"2025-04-21T14:09:37.685311Z","iopub.status.idle":"2025-04-21T14:09:37.713340Z","shell.execute_reply.started":"2025-04-21T14:09:37.685292Z","shell.execute_reply":"2025-04-21T14:09:37.712783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = test.drop(columns=[\"coldPresent\"])\ntest.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:37.714059Z","iopub.execute_input":"2025-04-21T14:09:37.714386Z","iopub.status.idle":"2025-04-21T14:09:37.721352Z","shell.execute_reply.started":"2025-04-21T14:09:37.714368Z","shell.execute_reply":"2025-04-21T14:09:37.720567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#new_data_dataset = new_data_dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:37.722282Z","iopub.execute_input":"2025-04-21T14:09:37.722534Z","iopub.status.idle":"2025-04-21T14:09:37.736925Z","shell.execute_reply.started":"2025-04-21T14:09:37.722511Z","shell.execute_reply":"2025-04-21T14:09:37.736224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Recreate the model instance (architecture must match)\n#model = MainModel(encoder=TransformerEncoderBlock(d_models=EMBED_DIM, num_heads=NUM_HEADS), classifier=Classifier())\n\n# Load the saved weights\n#model.load_weights(\"/kaggle/working/model_weights.weights.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:37.737509Z","iopub.execute_input":"2025-04-21T14:09:37.737714Z","iopub.status.idle":"2025-04-21T14:09:37.756003Z","shell.execute_reply.started":"2025-04-21T14:09:37.737698Z","shell.execute_reply":"2025-04-21T14:09:37.755280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dict = test.to_dict(orient=\"records\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:37.756834Z","iopub.execute_input":"2025-04-21T14:09:37.757110Z","iopub.status.idle":"2025-04-21T14:09:37.772436Z","shell.execute_reply.started":"2025-04-21T14:09:37.757082Z","shell.execute_reply":"2025-04-21T14:09:37.771639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prediksi data baru","metadata":{}},{"cell_type":"code","source":"# List untuk menyimpan hasil prediksi\nresults = []\n\nfor row in test_dict:\n    mfcc, features = process_row(row)  # Proses data\n    prediction = predict(model, mfcc, features)  # Prediksi\n\n    # Tambahkan hasil ke list\n    results.append({\n        \"candidateID\": row[\"candidateID\"],\n        \"disease\": prediction[0]\n    })\n\n# Konversi hasil ke DataFrame\nresults_df = pd.DataFrame(results)\n\n# Simpan ke CSV\noutput_path = \"/kaggle/working/predictions.csv\"\nresults_df.to_csv(output_path, index=False)\n\nprint(f\"Hasil prediksi disimpan di: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T14:09:37.773332Z","iopub.execute_input":"2025-04-21T14:09:37.773593Z","iopub.status.idle":"2025-04-21T14:11:01.675982Z","shell.execute_reply.started":"2025-04-21T14:09:37.773572Z","shell.execute_reply":"2025-04-21T14:11:01.675255Z"}},"outputs":[],"execution_count":null}]}